{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyleoclim as pyleo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining metadata and data pickle files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading metadata and data csv file and filtering only PAGES2kv2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading metadata\n",
    "\n",
    "df_meta = pd.read_csv('./proxydb_meta.csv')\n",
    "df_p2k_meta = df_meta[df_meta['Proxy ID'].str.contains('PAGES2kv2')]\n",
    "df_p2k_meta.set_index('Proxy ID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_data = df_p2k_meta[['Archive type', 'Proxy measurement', 'Lat (N)', 'Lon (E)', 'Elev']]\n",
    "archive_data['Proxy ID'] = archive_data.index  # recover it as a column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data (time and value)\n",
    "\n",
    "df = pd.read_csv('./proxydb_data.csv')\n",
    "col = df.columns\n",
    "\n",
    "clist = []\n",
    "for c in col:\n",
    "    if ('PAGES2kv2' in c) or ('Year C.E.' in c):\n",
    "        clist.append(c)\n",
    "\n",
    "p2k = df[clist]\n",
    "p2k = p2k.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for 'pid' column to follow CFR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col = p2k.columns[0]\n",
    "\n",
    "pids = []\n",
    "times = []\n",
    "values = []\n",
    "\n",
    "for pid in p2k.columns[1:]:  # Skip the time column\n",
    "        # Get the time series for this proxy\n",
    "        proxy_data = p2k[[time_col, pid]].dropna()\n",
    "        \n",
    "        if len(proxy_data) > 0:  # Only process if we have data\n",
    "            pids.append(pid)\n",
    "            times.append(proxy_data[time_col].tolist())\n",
    "            values.append(proxy_data[pid].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_pid(pids): \n",
    "    # Split by colon first to handle proxy measurements with underscores\n",
    "    parts_by_colon = pids.split(':')[0]  # Take everything before the colon\n",
    "    parts = parts_by_colon.split('_')\n",
    "    \n",
    "    # Find the part that contains the number\n",
    "    number = None\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            number = part\n",
    "            break\n",
    "        elif 'NAm_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "        elif 'Asi_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "        elif 'Arc_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "        elif 'Ant_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "        elif 'Ocn_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "        elif 'Aus_' in part:\n",
    "            number = part.split('_')[1]\n",
    "            break\n",
    "    \n",
    "    # Extract region from the second part\n",
    "    region = parts[1].split('-')[0]\n",
    "    \n",
    "    # Map the region\n",
    "    region_mapping = {\n",
    "        'Asia': 'Asi',\n",
    "        'Ocean2kHR': 'Ocn',\n",
    "        'Africa': 'Afr',\n",
    "        'Afr': 'Afr',\n",
    "        'NAm': 'NAm',\n",
    "        'Arc': 'Arc',\n",
    "        'Ant': 'Ant',\n",
    "        'Aus': 'Aus',\n",
    "        'SAm': 'SAm'\n",
    "    }\n",
    "    region = region_mapping.get(region, region)\n",
    "    \n",
    "    if number is None:\n",
    "        # If we still haven't found a number, look for it specifically\n",
    "        for part in parts:\n",
    "            if any(char.isdigit() for char in part):\n",
    "                number = ''.join(char for char in part if char.isdigit())\n",
    "                break\n",
    "    \n",
    "    return f\"{region}_{number}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pids = []\n",
    "\n",
    "for p in pids:\n",
    "    lil = shorten_pid(p)\n",
    "    short_pids.append(lil)\n",
    "\n",
    "df_p2k_meta_short = df_p2k_meta.rename(index=shorten_pid)\n",
    "archive_data_new_idx = archive_data.rename(index=shorten_pid)\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame({\n",
    "        'pid': short_pids,\n",
    "        'time': times,\n",
    "        'value': values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_data_new_idx = archive_data_new_idx.reset_index(names=['pid'])\n",
    "\n",
    "new_pdb = pd.merge(\n",
    "        archive_data_new_idx,\n",
    "        data_df,\n",
    "        on='pid',\n",
    "        how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ptype(row):\n",
    "\n",
    "    archive_mapping = {\n",
    "        'Tree Rings': 'tree',\n",
    "        'Corals and Sclerosponges': 'coral',\n",
    "        'Lake Cores': 'lake',\n",
    "        'Ice Cores': 'ice',\n",
    "        'Bivalve': 'bivalve',\n",
    "        'Speleothems': 'speleothem',\n",
    "        'Marine Cores': 'marine'\n",
    "    }\n",
    "\n",
    "    measure_mapping = {\n",
    "        'Sr_Ca': 'SrCa',\n",
    "        'trsgi': 'TRW',\n",
    "        'calcification': 'calc',\n",
    "        'd18O': 'd18O',\n",
    "        'dD': 'dD',\n",
    "        'MXD': 'MXD',\n",
    "        'density': 'MXD',\n",
    "        'composite': 'calc',\n",
    "        'massacum': 'accumulation',\n",
    "        'thickness': 'varve_thickness',\n",
    "        'melt': 'melt',\n",
    "        'RABD660_670': 'reflectance',\n",
    "        'X_radiograph_dark_layer': 'varve_property'\n",
    "    }\n",
    "    \n",
    "    # Get the shortened archive type\n",
    "    archive = archive_mapping.get(row['Archive type'], row['Archive type'].lower())\n",
    "    proxy = measure_mapping.get(row['Proxy measurement'])\n",
    "    \n",
    "    # Combine with proxy measurement\n",
    "    return f\"{archive}.{proxy}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pdb['ptype'] = new_pdb.apply(create_ptype, axis=1)\n",
    "new_pdb = new_pdb.drop(['Archive type', 'Proxy measurement'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making function for sub-annual filtering (from LMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_subannual(time_array):\n",
    "    if len(time_array) < 2:\n",
    "        return False\n",
    "    dt = np.median(np.diff(time_array))\n",
    "    return dt < 0.95  # annual resolution threshold (from LMR)\n",
    "\n",
    "def compute_annual_means_for_row(row, valid_frac=0.5):\n",
    "    pid = row['pid']\n",
    "\n",
    "    # Manually skip annualizing for this record\n",
    "    if pid == 'Ocn_148':\n",
    "        return row['time'], row['value'], False \n",
    "    \n",
    "    time = np.array(row['time'], dtype=float)\n",
    "    value = np.array(row['value'], dtype=float)\n",
    "\n",
    "    # Drop NaNs\n",
    "    mask = ~np.isnan(time) & ~np.isnan(value)\n",
    "    time = time[mask]\n",
    "    value = value[mask]\n",
    "\n",
    "    if len(time) < 2:\n",
    "        return None, None, False\n",
    "\n",
    "    subannual = is_subannual(time)\n",
    "\n",
    "    if subannual:\n",
    "        # Bin into annual averages\n",
    "        df = pd.DataFrame({'year': time.astype(int), 'value': value})\n",
    "        annual = df.groupby('year').mean().dropna()\n",
    "        \n",
    "        year_range = annual.index.max() - annual.index.min() + 1\n",
    "        coverage = len(annual) / year_range if year_range > 0 else 0\n",
    "        \n",
    "        if coverage < valid_frac:\n",
    "            return None, None, subannual\n",
    "        \n",
    "        return annual.index.to_list(), annual['value'].to_list(), subannual\n",
    "    \n",
    "    else:\n",
    "        # Assume already annual â€” check coverage\n",
    "        years = time.astype(int)\n",
    "        year_range = years.max() - years.min() + 1\n",
    "        coverage = len(years) / year_range if year_range > 0 else 0\n",
    "        \n",
    "        if coverage < valid_frac:\n",
    "            return None, None, subannual\n",
    "        \n",
    "        return years.tolist(), value.tolist(), subannual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = new_pdb.apply(lambda row: compute_annual_means_for_row(row), axis=1)\n",
    "\n",
    "new_pdb[['annual_time', 'annual_value', 'is_subannual']] = pd.DataFrame(results.tolist(), index=new_pdb.index)\n",
    "new_pdb = new_pdb[new_pdb['pid'] != 'Afr_012']\n",
    "\n",
    "# Drop records where annualization failed\n",
    "new_pdb_up = new_pdb[new_pdb['annual_time'].notnull()]\n",
    "\n",
    "\n",
    "new_pdb_up = new_pdb_up.drop(columns=['annual_time', 'annual_value', 'is_subannual', 'Proxy ID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pdb_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For use in cfr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pdb_up = new_pdb_up.rename(columns={\n",
    "    'Lat (N)': 'lat',\n",
    "    'Lon (E)': 'lon',\n",
    "    'Elev': 'elev'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pdb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pdb_up.to_json('./prev_data/updated_pages2kv2.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = cfr.ReconJob()\n",
    "job.load_proxydb(\n",
    "    path='./prev_data/updated_pages2kv2.json', \n",
    "    verbose=True,\n",
    "    pid_column='pid', \n",
    "    lat_column='lat',  \n",
    "    lon_column='lon',  \n",
    "    elev_column = 'elev',\n",
    "    time_column = 'time',\n",
    "    value_column = 'value',\n",
    "    ptype_column = 'ptype'\n",
    "\n",
    ")\n",
    "\n",
    "fig, ax = job.proxydb.plot(plot_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfr-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
